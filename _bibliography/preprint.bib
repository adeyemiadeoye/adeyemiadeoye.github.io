---
---

@misc{adeoye2025proximalaugmentedlagrangianmethod,
  bibtex_show={true},
  selected=true,
  abbr={arXiv},
  title={{A proximal augmented Lagrangian method for nonconvex optimization with equality and inequality constraints}},
  author={Adeyemi D. Adeoye and Puya Latafat and Alberto Bemporad},
  year={2025},
  eprint={2509.02894},
  archivePrefix={arXiv},
  primaryClass={math.OC},
  abstract={We propose an inexact proximal augmented Lagrangian method (P-ALM) for solving nonconvex structured optimization problems. The proposed method features an easily implementable rule not only for updating the penalty parameters, but also for adaptively tuning the proximal term. It allows the penalty parameter to grow rapidly in the early stages to speed up progress, while ameliorating the issue of ill-conditioning in later iterations, a well-known drawback of the traditional approach of linearly increasing the penalty parameters. A key element in our analysis lies in the observation that the augmented Lagrangian can be controlled effectively along the iterates, provided an initial feasible point is available. Our analysis, while simple, provides a new theoretical perspective about P-ALM and, as a by-product, results in similar convergence properties for its non-proximal variant, the classical augmented Lagrangian method (ALM). Numerical experiments, including convex and nonconvex problem instances, demonstrate the effectiveness of our approach.},
  journal={arXiv preprint arXiv:2509.02894},
  pdf={https://arxiv.org/pdf/2509.02894},
  html={https://arxiv.org/abs/2509.02894},
  code={https://github.com/adeyemiadeoye/p-balm}
}

@misc{adeoye2023self,
  bibtex_show={true},
  abbr={arXiv},
  selected=true,
  title={{Self-concordant Smoothing for Large-Scale Convex Composite Optimization}},
  author={Adeoye, Adeyemi D and Bemporad, Alberto},
  abstract={We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other may be nonsmooth. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as l1-regularization and group-lasso penalties. We prove the convergence of two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful for overparameterized machine learning models and in the mini-batch settings. Numerical examples on both synthetic and real datasets demonstrate the efficiency of our approach and its superiority over existing approaches. A Julia package implementing the proposed algorithms is available at https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl.},
  journal={arXiv preprint arXiv:2309.01781},
  year={2023},
  eprint={2309.01781},
  archivePrefix={arXiv},
  primaryClass={math.OC},
  pdf={https://arxiv.org/pdf/2309.01781.pdf},
  code={https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl},
  html={https://arxiv.org/abs/2309.01781},
}
