---
---

@misc{korbit2024exact,
  bibtex_show={true},
  selected=true,
  abbr={arXiv},
  title={Exact Gauss-Newton Optimization for Training Deep Neural Networks}, 
  author={Mikalai Korbit and Adeyemi D. Adeoye and Alberto Bemporad and Mario Zanon},
  year={2024},
  eprint={2405.14402},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  abstract={We present EGN, a stochastic second-order optimization algorithm that combines the generalized Gauss-Newton (GN) Hessian approximation with low-rank linear algebra to compute the descent direction. Leveraging the Duncan-Guttman matrix identity, the parameter update is obtained by factorizing a matrix which has the size of the mini-batch. This is particularly advantageous for large-scale machine learning problems where the dimension of the neural network parameter vector is several orders of magnitude larger than the batch size. Additionally, we show how improvements such as line search, adaptive regularization, and momentum can be seamlessly added to EGN to further accelerate the algorithm. Moreover, under mild assumptions, we prove that our algorithm converges to an Ïµ-stationary point at a linear rate. Finally, our numerical experiments demonstrate that EGN consistently exceeds, or at most matches the generalization performance of well-tuned SGD, Adam, and SGN optimizers across various supervised and reinforcement learning tasks.},
  journal={arXiv preprint arXiv:2405.14402},
  pdf={https://arxiv.org/pdf/2405.14402},
  html={https://arxiv.org/abs/2405.14402},
}

@misc{adeoye2023self,
  bibtex_show={true},
  abbr={arXiv},
  selected=true,
  title={Self-concordant Smoothing for Large-Scale Convex Composite Optimization},
  author={Adeoye, Adeyemi D and Bemporad, Alberto},
  abstract={We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other may be nonsmooth. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as l1-regularization and group-lasso penalties. We prove the convergence of two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful for overparameterized machine learning models and in the mini-batch settings. Numerical examples on both synthetic and real datasets demonstrate the efficiency of our approach and its superiority over existing approaches. A Julia package implementing the proposed algorithms is available at https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl.},
  journal={arXiv preprint arXiv:2309.01781},
  year={2023},
  eprint={2309.01781},
  archivePrefix={arXiv},
  primaryClass={math.OC},
  pdf={https://arxiv.org/pdf/2309.01781.pdf},
  code={https://github.com/adeyemiadeoye/SelfConcordantSmoothOptimization.jl},
  html={https://arxiv.org/abs/2309.01781},
}

@misc{adeoye2024regularized,
  bibtex_show={true},
  abbr={arXiv},
  selected=true,
  title={Regularized Gauss-Newton for Optimizing Overparameterized Neural Networks},
  author={Adeoye, Adeyemi D and Petersen, Philipp Christian and Bemporad, Alberto},
  abstract={The generalized Gauss-Newton (GGN) optimization method incorporates curvature estimates into its solution steps, and provides a good approximation to the Newton method for large-scale optimization problems. GGN has been found particularly interesting for practical training of deep neural networks, not only for its impressive convergence speed, but also for its close relation with neural tangent kernel regression, which is central to recent studies that aim to understand the optimization and generalization properties of neural networks. This work studies a GGN method for optimizing a two-layer neural network with explicit regularization. In particular, we consider a class of generalized self-concordant (GSC) functions that provide smooth approximations to commonly-used penalty terms in the objective function of the optimization problem. This approach provides an adaptive learning rate selection technique that requires little to no tuning for optimal performance. We study the convergence of the two-layer neural network, considered to be overparameterized, in the optimization loop of the resulting GGN method for a given scaling of the network parameters. Our numerical experiments highlight specific aspects of GSC regularization that help to improve generalization of the optimized neural network. The code to reproduce the experimental results is available at https://github.com/adeyemiadeoye/ggn-score-nn.},
  journal={arXiv preprint arXiv:2404.14875},
  year={2024},
  eprint={2404.14875},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  pdf={https://arxiv.org/pdf/2404.14875},
  code={https://github.com/adeyemiadeoye/ggn-score-nn},
  html={https://arxiv.org/abs/2404.14875}
}
