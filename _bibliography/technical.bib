---
---

@phdthesis{adeoyephdthesis2025,
  bibtex_show={true},
  abbr={PhD},
  title={Quasi-Newton methods for solving nonsmooth optimization problems in learning and control},
  author={Adeoye, Adeyemi Damilare},
  school={IMT School for Advanced Studies Lucca, Italy},
  abstract={This thesis is concerned with the design and analysis of some quasi-Newton methods for solving optimization problems in machine learning and control of nonlinear dynamical systems. The proposed algorithms are designed to exploit approximate second-order information to improve convergence rates, stability, and generalization of the learned models and control policies. The thesis is organized into two main parts. In the first part, we present a generalized Gauss-Newton algorithm that uses an adaptive step-size selection strategy and preserves the affine-invariant property of Newton's method. This algorithm significantly reduces the computational cost of Gauss-Newton methods, particularly in mini-batch supervised learning. We then extend this with a proximal method for nonsmooth convex composite optimization, resulting in two new algorithms. In the second part, we treat learning and control problems in the training of neural networks. First, we present a rigorous theoretical study of the generalized Gauss-Newton algorithm for the optimization of feedforward neural networks. This study establishes a non-asymptotic guarantee for the convergence of feedforward neural networks with a general explicit regularizer. Then, an inexact sequential quadratic programming framework is proposed for optimal control in recurrent neural networks, using a two-stage approach for system identification and optimal control policy selection. Several practical applications of all the proposed algorithms are demonstrated through numerical experiments.},
  year={2025},
  doi = {10.13118/imtlucca/e-theses/449},
  pdf={http://e-theses.imtlucca.it/449/1/Adeoye_phdthesis.pdf},
  html={http://e-theses.imtlucca.it/449/},
  slides={Adeyemi_thesis_slides.pdf}
}

@misc{adeoye2024regularized,
  bibtex_show={true},
  abbr={arXiv},
  selected=true,
  title={{Regularized Gauss-Newton for Optimizing Overparameterized Neural Networks}},
  author={Adeoye, Adeyemi D and Petersen, Philipp Christian and Bemporad, Alberto},
  abstract={The generalized Gauss-Newton (GGN) optimization method incorporates curvature estimates into its solution steps, and provides a good approximation to the Newton method for large-scale optimization problems. GGN has been found particularly interesting for practical training of deep neural networks, not only for its impressive convergence speed, but also for its close relation with neural tangent kernel regression, which is central to recent studies that aim to understand the optimization and generalization properties of neural networks. This work studies a GGN method for optimizing a two-layer neural network with explicit regularization. In particular, we consider a class of generalized self-concordant (GSC) functions that provide smooth approximations to commonly-used penalty terms in the objective function of the optimization problem. This approach provides an adaptive learning rate selection technique that requires little to no tuning for optimal performance. We study the convergence of the two-layer neural network, considered to be overparameterized, in the optimization loop of the resulting GGN method for a given scaling of the network parameters. Our numerical experiments highlight specific aspects of GSC regularization that help to improve generalization of the optimized neural network. The code to reproduce the experimental results is available at https://github.com/adeyemiadeoye/ggn-score-nn.},
  journal={arXiv preprint arXiv:2404.14875},
  year={2024},
  eprint={2404.14875},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  pdf={https://arxiv.org/pdf/2404.14875},
  code={https://github.com/adeyemiadeoye/ggn-score-nn},
  html={https://arxiv.org/abs/2404.14875}
}

@techreport{adeoye2021sc,
  bibtex_show={true},
  abbr={arXiv},
  title={SC-Reg: Training Overparameterized Neural Networks under Self-Concordant Regularization},
  author={Adeoye, Adeyemi D and Bemporad, Alberto},
  institution={IMT School for Advanced Studies Lucca},
  abstract={In this paper we propose the SC-Reg (self-concordant regularization) framework for learning overparameterized feedforward neural networks by incorporating second-order information in the Newton decrement framework for convex problems. We propose the generalized Gauss-Newton with Self-Concordant Regularization (SCoRe-GGN) algorithm that updates the network parameters each time it receives a new input batch. The proposed algorithm exploits the structure of the second-order information in the Hessian matrix, thereby reducing the training computational overhead. Although our current analysis considers only the convex case, numerical experiments show the efficiency of our method and its fast convergence under both convex and non-convex settings, which compare favorably against baseline first-order methods and a quasi-Newton method.},
  year={2021},
  doi = {10.48550/arXiv.2112.07344},
  pdf={https://arxiv.org/pdf/2112.07344v1.pdf},
  html={https://arxiv.org/abs/2112.07344v1},
}

@techreport{adeoye2021dnn,
  bibtex_show={true},
  abbr={MSc},
  title={A Deep Neural Network Optimization Method Via A Traffic Flow Model},
  author={Adeoye, Adeyemi Damilare and Petersen, Philipp},
  institution={African Institue for Mathematical Sciences, Rwanda},
  abstract={We present, via the solution of nonlinear parabolic partial differential equations (PDEs), a continuous-time formulation for stochastic optimization algorithms used for training deep neural networks. Using continuous-time formulation of stochastic differential equations (SDEs), relaxation approaches like the stochastic gradient descent (SGD) method are interpreted as the solution of nonlinear PDEs that arise from modeling physical problems. We reinterpret, through homogenization of SDEs, the modified SGD algorithm as the solution of the viscous Burgers' equation that models a highway traffic flow.},
  html={https://zenodo.org/records/17385893},
  doi={10.5281/zenodo.17385892},
  year={2021},
  pdf={adeoye-petersen-2021.pdf},
}

@mastersthesis{adeoye2018thesis,
  bibtex_show={true},
  abbr={MSc},
  title={Blood Flow in an Inclined Tapered Stenosed Porous Artery under the Influence of Magnetic Field and Heat Transfer},
  author={Adeoye, Adeyemi Damilare},
  institution={African Institute for Mathematical Sciences, Cameroon},
  abstract={A tapered inclined porous artery with stenosis was considered under the influence of magnetic field and heat transfer. The mathematical formulation for the momentum and energy equations of the blood flow considered to be Newtonian were obtained. The energy equation which was obtained by taking an extra factor of heat source and the nonlinear momentum equation were simplified under the assumption of mild stenosis. These equations were non-dimensionalized and solved using Differential Transform Method (DTM) to obtain expressions for velocity, temperature and volumetric flow rate. The graphs of the expressions were plotted against radius of the artery to simulate the effects of magnetic field, heat transfer and other fluid parameters on the velocity, temperature and the volumetric flow rate of the blood. It was observed that as the magnetic field parameter ($M$) increases, the velocity, temperature and the volumetric flow rate of the blood increase but wall shear stress decreases at the stenosis throat. It was further observed that the effects of heat transfer and magnetic field resulted into a greater variation in the volumetric flow of an inclined artery in the converging region than in the diverging region.},
  doi={10.5281/zenodo.17386590},
  year={2018},
  pdf={https://library.nexteinstein.org/wp-content/uploads/2023/01/Adeyemi-2017-2018.pdf},
  html={https://library.nexteinstein.org/thesis/blood-flow-in-an-inclined-tapered-stenosed-porous-artery-under-the-influence-of-magnetic-field-and-heat-transfer/},
}

@thesis{adeoye2016thesis,
  abbr={BSc},
  title={ON SOME FINITE DIFFERENCE METHODS FOR SOLVING PARTIAL DIFFERENTIAL EQUATIONS},
  author={Adeoye, Adeyemi Damilare},
  institution={University of Ilorin, Ilorin, Nigeria},
  year={2016}
}
